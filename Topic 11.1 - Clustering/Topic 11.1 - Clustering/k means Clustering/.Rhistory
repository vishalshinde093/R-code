install.packages("C:\Users\Vishal\Desktop\rhdfs_1.0.8")
install.packages("C:\\Users\\Vishal\\Desktop\\rhdfs_1.0.8")
install.packages("rJava")
install.packages("rJSONIO")
install.packages("RJSONIO")
install.packages("rmr")
install.packages("rMR")
install.packages("rhdfs")
install.packages("rmr2")
install.packages("rhdfs_1.0.8.tar.gz", repos=NULL, type="source")
install.packages("C:\\Users\\Vishal\\Desktop\\rhdfs_1.0.8.tar.gz", repos=NULL, type="source")
install.packages("C:\\Users\\Vishal\\Desktop\\rhdfs_1.0.8.zip", repos=NULL, type="source")
library(rhdfs)
grep("[au]",c("Harry Potter","Game of thrones","Lord of the rings"))
grep("t.e",c("Harry Potter","Game of thrones","Lord of the rings"))
regexpr("#",c("Harry #Potter","Game of #thrones","Lord of the #rings"))
sub("(Th+)", "e", c("The mountain The", "The hill hill",
"The city without pollution is The peaceful city","Thethe"), perl=TRUE)
sub("(Th+)", "\\1e", c("The mountain", "The hill", "The city without pollution is The peaceful city",
"Thethe"), perl=TRUE)
gsub("(Th+)", "e", c("The mountain The", "The hill hill", "The city without pollution is The peaceful city", "Thethe"), perl=TRUE)
#---------------------------------LDA : TopicModels-------------------------------------------#
library(RTextTools)
library(topicmodels)
library(tm)
data(NYTimes)
data <- NYTimes[sample(1:3100,size=1000,replace=FALSE),]
dim(data)
head(data)
View(data)
#Create a Document Term Matrix
matrix= create_matrix(cbind(as.vector(data$Title),as.vector(data$Subject)),
language="english",removeNumbers=TRUE, removePunctuation=TRUE,
removeSparseTerms=0,
removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
inspect(matrix[1:10,1:10])
inspect(matrix[1,1:10])
#Choose number of topics already existing
k <- length(unique(data$Topic.Code))
k
#Divide Data into training and testing
train <- matrix[1:700,]
test <- matrix[701:1000,]
#Building model on train data
train.lda <- LDA(train,k)
get_terms(train.lda,5)
terms(train.lda)
topics(train.lda)
terms(train.lda)
topics(train.lda)
#Get the top topics
train.topics <- topics(train.lda)
#Testing the model
test.topics <- posterior(train.lda,test)
test.topics$topics[1:10,1:5]
test.topics$topics[1:10,1:5]
test.topics <- apply(test.topics$topics, 1, which.max)
#Joining the predicted Topic number to the original test Data
test<-data[701:1000,]
final<-data.frame(Title=test$Title,Subject=test$Subject,Pred_topic=test.topics)
View(final)
#Analysis
table(final$Pred_topic)
View(final[final$Pred_topic==22,])
##--------------------Another method to choose the optimal number of topics ---------#
#------Checking best number of topics--------#
library(topicmodels)
best.model <- lapply(seq(2,10, by=1), function(k){LDA(matrix,k)})
best_model<- as.data.frame(as.matrix(lapply(best.model, logLik)))
final_best_model <- data.frame(topics=c(seq(2,10, by=1)),
log_likelihood=as.numeric(as.matrix(best_model)))
head(final_best_model)
library(ggplot2)
with(final_best_model,qplot(topics,log_likelihood,color="red"))
#Based on the graph, we can choose the best model
k=final_best_model[which.max(final_best_model$log_likelihood),1]
cat("Best topic number is k=",k)
#####Predicting Outcome of Titanic Using GBM & GLMNET
#install.packages("caret",dep=T)
#install.packages("pROC", dep=T)
# load libraries
library(caret)
library(pROC)
#################################################
# data prep
#################################################
# load data
titanicDF <- read.csv("C:/Users/Vishal/Desktop/jigsaw/bagging and boosting/Titanic Case Study/titanicDF.csv")
View(titanicDF)
titanicDF$Title <- ifelse(grepl('Mr ',titanicDF$Name),'Mr',ifelse(grepl('Mrs ',titanicDF$Name),'Mrs',ifelse(grepl('Miss',titanicDF$Name),'Miss','Nothing')))
titanicDF$Age[is.na(titanicDF$Age)] <- median(titanicDF$Age, na.rm=T)
# format
titanicDF <- titanicDF[c('PClass', 'Age',    'Sex',   'Title', 'Survived')]
# dummy variables for factors/characters
titanicDF$Title <- as.factor(titanicDF$Title)
titanicDummy <- dummyVars("~.",data=titanicDF, fullRank=F)
titanicDF <- as.data.frame(predict(titanicDummy,titanicDF))
print(names(titanicDF))
# what is the proportion of your outcome variable?
prop.table(table(titanicDF$Survived))
# save the outcome for the glmnet model
tempOutcome <- titanicDF$Survived
# generalize outcome and predictor variables
outcomeName <- 'Survived'
predictorsNames <- names(titanicDF)[names(titanicDF) != outcomeName]
#################################################
# model it
#################################################
# get names of all caret supported models
names(getModelInfo())
titanicDF$Survived <- ifelse(titanicDF$Survived==1,'yes','nope')
# pick model gbm and find out what type of model it is
getModelInfo()$gbm$type
# split data into training and testing chunks
set.seed(1234)
splitIndex <- createDataPartition(titanicDF[,outcomeName], p = .75, list = FALSE, times = 1)
trainDF <- titanicDF[ splitIndex,]
testDF  <- titanicDF[-splitIndex,]
# create caret trainControl object to control the number of cross-validations performed
objControl <- trainControl(method='cv', number=3, returnResamp='none', summaryFunction = twoClassSummary, classProbs = TRUE)
# run model
objModel <- train(trainDF[,predictorsNames], as.factor(trainDF[,outcomeName]),
method='gbm',
trControl=objControl,
metric = "ROC",
preProc = c("center", "scale"))
# find out variable importance
summary(objModel)
# find out model details
objModel
#################################################
# evalutate model
#################################################
# get predictions on your testing data
# class prediction
predictions <- predict(object=objModel, testDF[,predictorsNames], type='raw')
head(predictions)
postResample(pred=predictions, obs=as.factor(testDF[,outcomeName]))
# probabilities
predictions <- predict(object=objModel, testDF[,predictorsNames], type='prob')
head(predictions)
postResample(pred=predictions[[2]], obs=ifelse(testDF[,outcomeName]=='yes',1,0))
auc <- roc(ifelse(testDF[,outcomeName]=="yes",1,0), predictions[[2]])
print(auc$auc)
################################################
# glmnet model
################################################
# pick model gbm and find out what type of model it is
getModelInfo()$glmnet$type
# save the outcome for the glmnet model
titanicDF$Survived  <- tempOutcome
# split data into training and testing chunks
set.seed(1234)
splitIndex <- createDataPartition(titanicDF[,outcomeName], p = .75, list = FALSE, times = 1)
trainDF <- titanicDF[ splitIndex,]
testDF  <- titanicDF[-splitIndex,]
# create caret trainControl object to control the number of cross-validations performed
objControl <- trainControl(method='cv', number=3, returnResamp='none')
# run model
objModel <- train(trainDF[,predictorsNames], trainDF[,outcomeName], method='glmnet',  metric = "RMSE", trControl=objControl)
# get predictions on your testing data
predictions <- predict(object=objModel, testDF[,predictorsNames])
library(pROC)
auc <- roc(testDF[,outcomeName], predictions)
print(auc$auc)
postResample(pred=predictions, obs=testDF[,outcomeName])
# find out variable importance
summary(objModel)
plot(varImp(objModel,scale=F))
# find out model details
objModel
?RMSE
#install.packages("mlbench", dep=T)
#install.packages("caret", dep=T)
# load libraries
library(mlbench)
library(caret)
?trainControl(method="repeatedcv", number=10, repeats=3)
# load the dataset
data(PimaIndiansDiabetes)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
control
#install.packages("mlbench", dep=T)
#install.packages("caret", dep=T)
# load libraries
library(mlbench)
library(caret)
# load the dataset
data(PimaIndiansDiabetes)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# CART
set.seed(7)
fit.cart <- train(diabetes~., data=PimaIndiansDiabetes, method="rpart", trControl=control)
# LDA
set.seed(7)
fit.lda <- train(diabetes~., data=PimaIndiansDiabetes, method="lda", trControl=control)
# SVM
set.seed(7)
fit.svm <- train(diabetes~., data=PimaIndiansDiabetes, method="svmRadial", trControl=control)
# kNN
set.seed(7)
fit.knn <- train(diabetes~., data=PimaIndiansDiabetes, method="knn", trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(diabetes~., data=PimaIndiansDiabetes, method="rf", trControl=control)
#GBM
set.seed(7)
fit.gbm <- train(diabetes~., data=PimaIndiansDiabetes, method="gbm", trControl=control, verbose=FALSE)
# collect resamples
results <- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf,GBM=fit.gbm))
# summarize differences between modes
summary(results)
library(mlbench)
library(caret)
# load the dataset
data(PimaIndiansDiabetes)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=4)
# CART
set.seed(7)
fit.cart <- train(diabetes~., data=PimaIndiansDiabetes, method="rpart", trControl=control)
# LDA
set.seed(7)
fit.lda <- train(diabetes~., data=PimaIndiansDiabetes, method="lda", trControl=control)
# SVM
set.seed(7)
fit.svm <- train(diabetes~., data=PimaIndiansDiabetes, method="svmRadial", trControl=control)
# kNN
set.seed(7)
fit.knn <- train(diabetes~., data=PimaIndiansDiabetes, method="knn", trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(diabetes~., data=PimaIndiansDiabetes, method="rf", trControl=control)
#GBM
set.seed(7)
fit.gbm <- train(diabetes~., data=PimaIndiansDiabetes, method="gbm", trControl=control, verbose=FALSE)
# collect resamples
results <- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf,GBM=fit.gbm))
# summarize differences between modes
summary(results)
# load libraries
library(mlbench)
library(caret)
# load the dataset
data(PimaIndiansDiabetes)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# CART
set.seed(7)
fit.cart <- train(diabetes~., data=PimaIndiansDiabetes, method="rpart", trControl=control)
# LDA
set.seed(7)
fit.lda <- train(diabetes~., data=PimaIndiansDiabetes, method="lda", trControl=control)
# SVM
set.seed(7)
fit.svm <- train(diabetes~., data=PimaIndiansDiabetes, method="svmRadial", trControl=control)
# kNN
set.seed(7)
fit.knn <- train(diabetes~., data=PimaIndiansDiabetes, method="knn", trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(diabetes~., data=PimaIndiansDiabetes, method="rf", trControl=control)
#GBM
set.seed(7)
fit.gbm <- train(diabetes~., data=PimaIndiansDiabetes, method="gbm", trControl=control, verbose=FALSE)
# collect resamples
results <- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf,GBM=fit.gbm))
# summarize differences between modes
summary(results)
?resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf,GBM=fit.gbm))
# box and whisker plots to compare models
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(results, scales=scales)
# density plots of accuracy
scales <- list(x=list(relation="free"), y=list(relation="free"))
densityplot(results, scales=scales, pch = "|")
# dot plots of accuracy
scales <- list(x=list(relation="free"), y=list(relation="free"))
dotplot(results, scales=scales)
# difference in model predictions
diffs <- diff(results)
# summarize p-values for pair-wise comparisons
summary(diffs)
#---------------------------------LDA : TopicModels-------------------------------------------#
library(RTextTools)
library(topicmodels)
library(tm)
data(NYTimes)
data <- NYTimes[sample(1:3100,size=1000,replace=FALSE),]
dim(data)
head(data)
#Create a Document Term Matrix
matrix= create_matrix(cbind(as.vector(data$Title),as.vector(data$Subject)),
language="english",removeNumbers=TRUE, removePunctuation=TRUE,
removeSparseTerms=0,
removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
inspect(matrix[1:10,1:10])
#---------------------------------LDA : TopicModels-------------------------------------------#
library(RTextTools)
library(topicmodels)
library(tm)
data(NYTimes)
data <- NYTimes[sample(1:3100,size=1000,replace=FALSE),]
dim(data)
head(data)
#Create a Document Term Matrix
matrix= create_matrix(cbind(as.vector(data$Title),as.vector(data$Subject)),
language="english",removeNumbers=TRUE, removePunctuation=TRUE,
removeSparseTerms=0,
removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
inspect(matrix[1:10,1:10])
#Choose number of topics already existing
k <- length(unique(data$Topic.Code))
k
#Divide Data into training and testing
train <- matrix[1:700,]
test <- matrix[701:1000,]
#Building model on train data
train.lda <- LDA(train,k)
train.lda <- LDA(train,k)
terms(train.lda)
topics(train.lda)
write.csv(data.frame(get_topics(train.lda,27)),"terms_nytimes1.csv",row.names=F)
View(NYTimes)
View(data)
train.topics <- topics(train.lda)
View(data)
View(data)
?fit.km$withinss
setwd("C:/Users/Vishal/Desktop/jigsaw/clustering/Topic 11.1 - Clustering/Topic 11.1 - Clustering/k means Clustering")
##Clustering in R
#Read and explore the data
##--------------------------------------Step1 : Reading and exploring the dataset ---------------------------------------------
data<-read.csv("cust.csv")
dim(data)
str(data)
names(data)
summary(data)
colSums(is.na(data))
#Frequency distribution
table(data$Channel)
table(data$Region)
#Subsetting Data to only Price ,Bedrooms,Squarefeet
names(data)
sample<-data[,3:8]
dim(sample)
str(sample)
names(sample)
##--------------------------------------Step2 : Scaling the data ---------------------------------------------
#(column  - mean(column))/sd(column)
#Repeat for all columns
list<-names(sample)
scaled_data<-data.frame(rownum<-1:440)
for(i in 1:length(list))
{
x<-(sample[,i]-mean(sample[,i]))/(sd(sample[,i]))
scaled_data<-cbind(scaled_data,x)
names(scaled_data)[i+1]<-paste("scaled_",list[i])
print(list[i])
}
head(scaled_data)
scaled_data<-scaled_data[,-1]
#
sample<-cbind(sample,scaled_data)
names(sample)
# head(sample)
##--------------------------------------Step3 : kmeans algorithm ---------------------------------------------
#syntax : kmeans(scaled_data,k) ; where k refers to the number of clusters
set.seed(200)
names(sample)
fit.km<-kmeans(sample[,7:12],3)
#We will get a list object
fit.km$size
?fit.km$withinss
?fit.km$withinss #Within sum of squares metric for each cluster
?withinss
??withinss
