install.packages("C:\\Users\\Vishal\\Desktop\\rhdfs_1.0.8.zip", repos=NULL, type="source")
library(rhdfs)
grep("[au]",c("Harry Potter","Game of thrones","Lord of the rings"))
grep("t.e",c("Harry Potter","Game of thrones","Lord of the rings"))
regexpr("#",c("Harry #Potter","Game of #thrones","Lord of the #rings"))
sub("(Th+)", "e", c("The mountain The", "The hill hill",
"The city without pollution is The peaceful city","Thethe"), perl=TRUE)
sub("(Th+)", "\\1e", c("The mountain", "The hill", "The city without pollution is The peaceful city",
"Thethe"), perl=TRUE)
gsub("(Th+)", "e", c("The mountain The", "The hill hill", "The city without pollution is The peaceful city", "Thethe"), perl=TRUE)
#---------------------------------LDA : TopicModels-------------------------------------------#
library(RTextTools)
library(topicmodels)
library(tm)
data(NYTimes)
data <- NYTimes[sample(1:3100,size=1000,replace=FALSE),]
dim(data)
head(data)
View(data)
#Create a Document Term Matrix
matrix= create_matrix(cbind(as.vector(data$Title),as.vector(data$Subject)),
language="english",removeNumbers=TRUE, removePunctuation=TRUE,
removeSparseTerms=0,
removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
inspect(matrix[1:10,1:10])
inspect(matrix[1,1:10])
#Choose number of topics already existing
k <- length(unique(data$Topic.Code))
k
#Divide Data into training and testing
train <- matrix[1:700,]
test <- matrix[701:1000,]
#Building model on train data
train.lda <- LDA(train,k)
get_terms(train.lda,5)
terms(train.lda)
topics(train.lda)
terms(train.lda)
topics(train.lda)
#Get the top topics
train.topics <- topics(train.lda)
#Testing the model
test.topics <- posterior(train.lda,test)
test.topics$topics[1:10,1:5]
test.topics$topics[1:10,1:5]
test.topics <- apply(test.topics$topics, 1, which.max)
#Joining the predicted Topic number to the original test Data
test<-data[701:1000,]
final<-data.frame(Title=test$Title,Subject=test$Subject,Pred_topic=test.topics)
View(final)
#Analysis
table(final$Pred_topic)
View(final[final$Pred_topic==22,])
##--------------------Another method to choose the optimal number of topics ---------#
#------Checking best number of topics--------#
library(topicmodels)
best.model <- lapply(seq(2,10, by=1), function(k){LDA(matrix,k)})
best_model<- as.data.frame(as.matrix(lapply(best.model, logLik)))
final_best_model <- data.frame(topics=c(seq(2,10, by=1)),
log_likelihood=as.numeric(as.matrix(best_model)))
head(final_best_model)
library(ggplot2)
with(final_best_model,qplot(topics,log_likelihood,color="red"))
#Based on the graph, we can choose the best model
k=final_best_model[which.max(final_best_model$log_likelihood),1]
cat("Best topic number is k=",k)
#####Predicting Outcome of Titanic Using GBM & GLMNET
#install.packages("caret",dep=T)
#install.packages("pROC", dep=T)
# load libraries
library(caret)
library(pROC)
#################################################
# data prep
#################################################
# load data
titanicDF <- read.csv("C:/Users/Vishal/Desktop/jigsaw/bagging and boosting/Titanic Case Study/titanicDF.csv")
View(titanicDF)
titanicDF$Title <- ifelse(grepl('Mr ',titanicDF$Name),'Mr',ifelse(grepl('Mrs ',titanicDF$Name),'Mrs',ifelse(grepl('Miss',titanicDF$Name),'Miss','Nothing')))
titanicDF$Age[is.na(titanicDF$Age)] <- median(titanicDF$Age, na.rm=T)
# format
titanicDF <- titanicDF[c('PClass', 'Age',    'Sex',   'Title', 'Survived')]
# dummy variables for factors/characters
titanicDF$Title <- as.factor(titanicDF$Title)
titanicDummy <- dummyVars("~.",data=titanicDF, fullRank=F)
titanicDF <- as.data.frame(predict(titanicDummy,titanicDF))
print(names(titanicDF))
# what is the proportion of your outcome variable?
prop.table(table(titanicDF$Survived))
# save the outcome for the glmnet model
tempOutcome <- titanicDF$Survived
# generalize outcome and predictor variables
outcomeName <- 'Survived'
predictorsNames <- names(titanicDF)[names(titanicDF) != outcomeName]
#################################################
# model it
#################################################
# get names of all caret supported models
names(getModelInfo())
titanicDF$Survived <- ifelse(titanicDF$Survived==1,'yes','nope')
# pick model gbm and find out what type of model it is
getModelInfo()$gbm$type
# split data into training and testing chunks
set.seed(1234)
splitIndex <- createDataPartition(titanicDF[,outcomeName], p = .75, list = FALSE, times = 1)
trainDF <- titanicDF[ splitIndex,]
testDF  <- titanicDF[-splitIndex,]
# create caret trainControl object to control the number of cross-validations performed
objControl <- trainControl(method='cv', number=3, returnResamp='none', summaryFunction = twoClassSummary, classProbs = TRUE)
# run model
objModel <- train(trainDF[,predictorsNames], as.factor(trainDF[,outcomeName]),
method='gbm',
trControl=objControl,
metric = "ROC",
preProc = c("center", "scale"))
# find out variable importance
summary(objModel)
# find out model details
objModel
#################################################
# evalutate model
#################################################
# get predictions on your testing data
# class prediction
predictions <- predict(object=objModel, testDF[,predictorsNames], type='raw')
head(predictions)
postResample(pred=predictions, obs=as.factor(testDF[,outcomeName]))
# probabilities
predictions <- predict(object=objModel, testDF[,predictorsNames], type='prob')
head(predictions)
postResample(pred=predictions[[2]], obs=ifelse(testDF[,outcomeName]=='yes',1,0))
auc <- roc(ifelse(testDF[,outcomeName]=="yes",1,0), predictions[[2]])
print(auc$auc)
################################################
# glmnet model
################################################
# pick model gbm and find out what type of model it is
getModelInfo()$glmnet$type
# save the outcome for the glmnet model
titanicDF$Survived  <- tempOutcome
# split data into training and testing chunks
set.seed(1234)
splitIndex <- createDataPartition(titanicDF[,outcomeName], p = .75, list = FALSE, times = 1)
trainDF <- titanicDF[ splitIndex,]
testDF  <- titanicDF[-splitIndex,]
# create caret trainControl object to control the number of cross-validations performed
objControl <- trainControl(method='cv', number=3, returnResamp='none')
# run model
objModel <- train(trainDF[,predictorsNames], trainDF[,outcomeName], method='glmnet',  metric = "RMSE", trControl=objControl)
# get predictions on your testing data
predictions <- predict(object=objModel, testDF[,predictorsNames])
library(pROC)
auc <- roc(testDF[,outcomeName], predictions)
print(auc$auc)
postResample(pred=predictions, obs=testDF[,outcomeName])
# find out variable importance
summary(objModel)
plot(varImp(objModel,scale=F))
# find out model details
objModel
?RMSE
#install.packages("mlbench", dep=T)
#install.packages("caret", dep=T)
# load libraries
library(mlbench)
library(caret)
?trainControl(method="repeatedcv", number=10, repeats=3)
# load the dataset
data(PimaIndiansDiabetes)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
control
#install.packages("mlbench", dep=T)
#install.packages("caret", dep=T)
# load libraries
library(mlbench)
library(caret)
# load the dataset
data(PimaIndiansDiabetes)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# CART
set.seed(7)
fit.cart <- train(diabetes~., data=PimaIndiansDiabetes, method="rpart", trControl=control)
# LDA
set.seed(7)
fit.lda <- train(diabetes~., data=PimaIndiansDiabetes, method="lda", trControl=control)
# SVM
set.seed(7)
fit.svm <- train(diabetes~., data=PimaIndiansDiabetes, method="svmRadial", trControl=control)
# kNN
set.seed(7)
fit.knn <- train(diabetes~., data=PimaIndiansDiabetes, method="knn", trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(diabetes~., data=PimaIndiansDiabetes, method="rf", trControl=control)
#GBM
set.seed(7)
fit.gbm <- train(diabetes~., data=PimaIndiansDiabetes, method="gbm", trControl=control, verbose=FALSE)
# collect resamples
results <- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf,GBM=fit.gbm))
# summarize differences between modes
summary(results)
library(mlbench)
library(caret)
# load the dataset
data(PimaIndiansDiabetes)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=4)
# CART
set.seed(7)
fit.cart <- train(diabetes~., data=PimaIndiansDiabetes, method="rpart", trControl=control)
# LDA
set.seed(7)
fit.lda <- train(diabetes~., data=PimaIndiansDiabetes, method="lda", trControl=control)
# SVM
set.seed(7)
fit.svm <- train(diabetes~., data=PimaIndiansDiabetes, method="svmRadial", trControl=control)
# kNN
set.seed(7)
fit.knn <- train(diabetes~., data=PimaIndiansDiabetes, method="knn", trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(diabetes~., data=PimaIndiansDiabetes, method="rf", trControl=control)
#GBM
set.seed(7)
fit.gbm <- train(diabetes~., data=PimaIndiansDiabetes, method="gbm", trControl=control, verbose=FALSE)
# collect resamples
results <- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf,GBM=fit.gbm))
# summarize differences between modes
summary(results)
# load libraries
library(mlbench)
library(caret)
# load the dataset
data(PimaIndiansDiabetes)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# CART
set.seed(7)
fit.cart <- train(diabetes~., data=PimaIndiansDiabetes, method="rpart", trControl=control)
# LDA
set.seed(7)
fit.lda <- train(diabetes~., data=PimaIndiansDiabetes, method="lda", trControl=control)
# SVM
set.seed(7)
fit.svm <- train(diabetes~., data=PimaIndiansDiabetes, method="svmRadial", trControl=control)
# kNN
set.seed(7)
fit.knn <- train(diabetes~., data=PimaIndiansDiabetes, method="knn", trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(diabetes~., data=PimaIndiansDiabetes, method="rf", trControl=control)
#GBM
set.seed(7)
fit.gbm <- train(diabetes~., data=PimaIndiansDiabetes, method="gbm", trControl=control, verbose=FALSE)
# collect resamples
results <- resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf,GBM=fit.gbm))
# summarize differences between modes
summary(results)
?resamples(list(CART=fit.cart, LDA=fit.lda, SVM=fit.svm, KNN=fit.knn, RF=fit.rf,GBM=fit.gbm))
# box and whisker plots to compare models
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(results, scales=scales)
# density plots of accuracy
scales <- list(x=list(relation="free"), y=list(relation="free"))
densityplot(results, scales=scales, pch = "|")
# dot plots of accuracy
scales <- list(x=list(relation="free"), y=list(relation="free"))
dotplot(results, scales=scales)
# difference in model predictions
diffs <- diff(results)
# summarize p-values for pair-wise comparisons
summary(diffs)
#---------------------------------LDA : TopicModels-------------------------------------------#
library(RTextTools)
library(topicmodels)
library(tm)
data(NYTimes)
data <- NYTimes[sample(1:3100,size=1000,replace=FALSE),]
dim(data)
head(data)
#Create a Document Term Matrix
matrix= create_matrix(cbind(as.vector(data$Title),as.vector(data$Subject)),
language="english",removeNumbers=TRUE, removePunctuation=TRUE,
removeSparseTerms=0,
removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
inspect(matrix[1:10,1:10])
#---------------------------------LDA : TopicModels-------------------------------------------#
library(RTextTools)
library(topicmodels)
library(tm)
data(NYTimes)
data <- NYTimes[sample(1:3100,size=1000,replace=FALSE),]
dim(data)
head(data)
#Create a Document Term Matrix
matrix= create_matrix(cbind(as.vector(data$Title),as.vector(data$Subject)),
language="english",removeNumbers=TRUE, removePunctuation=TRUE,
removeSparseTerms=0,
removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
inspect(matrix[1:10,1:10])
#Choose number of topics already existing
k <- length(unique(data$Topic.Code))
k
#Divide Data into training and testing
train <- matrix[1:700,]
test <- matrix[701:1000,]
#Building model on train data
train.lda <- LDA(train,k)
train.lda <- LDA(train,k)
terms(train.lda)
topics(train.lda)
write.csv(data.frame(get_topics(train.lda,27)),"terms_nytimes1.csv",row.names=F)
View(NYTimes)
View(data)
train.topics <- topics(train.lda)
View(data)
View(data)
df1 = data.frame(custy=c(1:6),cust=c(1:6));
df2= data.frame(custy=(mean(custy),median(custy),sd(custy),cust=(mean(cust),median(cust),sd(cust));
df2= data.frame(custy1=(mean(custy),median(custy),sd(custy),cust1=(mean(cust),median(cust),sd(cust));
df2= data.frame(custy1=(mean(custy),median(custy),sd(custy)),cust1=(mean(cust),median(cust),sd(cust)));
df2= data.frame(    custy1=c(mean(custy),median(custy),sd(custy)),
cust1=c(  mean(cust),median(cust),sd(cust)   ));
df2= data.frame(    custy1=c(mean(custy),median(custy),sd(custy)),
cust1=c(  mean(cust),median(cust),sd(cust)   ));
df2= data.frame(    custy1=c(mean(df1$custy),median(df1$custy),sd(df1$custy)  ),
cust1=c(  mean(df1$cust),median(df1$cust),sd(df1$cust)   ));
View(df2)
df1 = data.frame(custy=c(1:6),cust=c(1:6));
df2= data.frame(    custy1=c(mean(df1$custy),median(df1$custy),sd(df1$custy)  ),
cust1=c(  mean(df1$cust),median(df1$cust),sd(df1$cust)   ));
View(df2)
rownames(df2)=c(x,y,z);
rownames(df2)=c("x","y","z");
colnames(df2)=c("a","b");
x=1
if (x==1) then
{
y=4
}
else
{
y=9
}
x=1
if (x == 1)
{
y=4
} else
{
y=9
}
max(4,8)
setwd("C:/Users/Vishal/Desktop/jigsaw/clustering/Topic 11.1 - Clustering/Topic 11.1 - Clustering/k means Clustering")
##Clustering in R
#Read and explore the data
##--------------------------------------Step1 : Reading and exploring the dataset ---------------------------------------------
data<-read.csv("cust.csv")
dim(data)
str(data)
names(data)
summary(data)
colSums(is.na(data))
#Frequency distribution
table(data$Channel)
table(data$Region)
#Subsetting Data to only Price ,Bedrooms,Squarefeet
names(data)
sample<-data[,3:8]
dim(sample)
str(sample)
names(sample)
##--------------------------------------Step2 : Scaling the data ---------------------------------------------
#(column  - mean(column))/sd(column)
#Repeat for all columns
list<-names(sample)
scaled_data<-data.frame(rownum<-1:440)
for(i in 1:length(list))
{
x<-(sample[,i]-mean(sample[,i]))/(sd(sample[,i]))
scaled_data<-cbind(scaled_data,x)
names(scaled_data)[i+1]<-paste("scaled_",list[i])
print(list[i])
}
head(scaled_data)
scaled_data<-scaled_data[,-1]
#
sample<-cbind(sample,scaled_data)
names(sample)
# head(sample)
##--------------------------------------Step3 : kmeans algorithm ---------------------------------------------
#syntax : kmeans(scaled_data,k) ; where k refers to the number of clusters
set.seed(200)
names(sample)
fit.km<-kmeans(sample[,7:12],3)
#We will get a list object
fit.km$size #No of observations in each cluster
fit.km$withinss #Within sum of squares metric for each cluster
fit.km$totss #The total sum of squares
fit.km$tot.withinss #Total within-cluster sum of squares, i.e., sum(withinss)
fit.km$betweenss #The between-cluster sum of squares, i.e. totss-tot.withinss
##--------------------------------------Step4 : find the optimal number of clusters (k value) ---------------------------------------------
#Create a screeplot-plot of cluster's tot.withinss wrt number of clusters
wss<-1:15
number<-1:15
for (i in 1:15)
{
wss[i]<-kmeans(sample[,7:12],i)$tot.withinss
}
#Shortlised optimal number of clusters : between 7 and 9
#Better plot using ggplot2
library(ggplot2)
data_wss<-data.frame(wss,number)
p<-ggplot(data_wss,aes(x=number,y=wss),color="red")
p+geom_point()+scale_x_continuous(breaks=seq(1,20,1))
##--------------------------------------Step5a : Rerun the algorithm with k=7(optimal no)---------------------------------------------
#Build 9 cluster model
set.seed(100)
fit.km<-kmeans(sample[,7:12],9)
##Merging the cluster output with original data
sample$cluster<-fit.km$cluster
##--------------------------------------Step5b : Profile the clusters---------------------------------------------
#Cluster wise Aggregates
cmeans<-aggregate(sample[,1:6],by=list(sample$cluster),FUN=mean)
cmeans
dim(cmeans)
#Population Aggregates
apply(sample[,1:6],2,mean)
apply(sample[,1:6],2,sd)
list1<-names(cmeans)
View(cmeans)
#Z score calculation
#z score =population_mean - group_mean /population_sd
for(i in 1:length(list1))
{
y<-(cmeans[,i+1] - apply(sample[,1:6],2,mean)[i])/(apply(sample[,1:6],2,sd)[i])
cmeans<-cbind(cmeans,y)
names(cmeans)[i+7]<-paste("z",list1[i+1],sep="_")
print(list1[i+1])
}
cmeans<-cmeans[,-14]
names(cmeans)
setwd("C:/Users/Vishal/Desktop/jigsaw/clustering/Topic 11.1 - Clustering/Topic 11.1 - Clustering/Heirarchichal Clustering")
options(scipen=10)
##--------------------------------------Step1 : Read the data---------------------------------------------
europe<-read.csv("europe.csv")
names(europe)
dim(europe)
##--------------------------------------Step2 : Scaling data + Finding Distance Measures---------------------------------------------
#Distance calculation : Euclidean
#Method : Average
#Daisy function in R scales the data and computes the distance measures
library(cluster)
#?daisy
dmatrix<-daisy(europe[-1],metric="euclidean",stand=TRUE)
summary(dmatrix)
class(dmatrix)
distmatrix<-dist(dmatrix)
str(distmatrix)
#Writing out the file
d<-as.matrix(distmatrix)
class(d)
write.csv(d,"distmatrix.csv",row.names=F)
##--------------------------------------Step3 : Hierarchcal clusterting algorithm---------------------------------------------
#hclust
?hclust
euroclust<-hclust(distmatrix,method="average")
str(euroclust)
#euroclust<-hclust(distmatrix,method="complete")
##--------------------------------------Step4 : Plotting a Dendogram---------------------------------------------
plot(as.dendrogram(euroclust))
#plot(as.dendrogram(euroclust),labels=europe$Country)
rect.hclust(euroclust, 5)
##--------------------------------------Step5 : Examining the hclust object---------------------------------------------
#The cluster height used in the dendogram
euroclust$height
#labels for each of the objects being clustered
euroclust$labels<-europe$Country
#distance measure used
euroclust$dist.method
##----------------------------------Step6 : Slicing the dendogram to get finite number of clusters---------------------------------------------
#To get flat clustering :
k<-cutree(euroclust,k = 5)
head(k)
#Once you have k which is the cluster no, attach it to your dataset
europe$cluster<-k
#Cluster wise Summaries
cmeans<-aggregate(europe[,2:8],
by=list(europe$cluster),
FUN=mean)
names(cmeans)
dim(cmeans)
View(cmeans)
#Population Summaries
options(scipen=999)
apply(europe[,2:8],2,mean)
apply(europe[,2:8],2,sd)
#Z value normalisation
#Function to calculate Z values
list<-names(cmeans)
#z score =population_mean - group_mean /population_sd
for(i in 1:length(list))
{
y<-(cmeans[,i+1] - apply(europe[,2:8],2,mean)[i])/(apply(europe[,2:8],2,sd)[i])
cmeans<-cbind(cmeans,y)
names(cmeans)[i+8]<-paste("z",list[i+1],sep="_")
print(list[i+1])
}
cmeans<-cmeans[,-16]
write.csv(cmeans,"cmeans.csv",row.names=F)
